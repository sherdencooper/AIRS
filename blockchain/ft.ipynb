{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jvy5516/miniconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jvy5516/miniconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jvy5516/miniconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jvy5516/miniconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jvy5516/miniconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jvy5516/miniconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_3818191/2834747658.py:19: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "import os\n",
    "from environment import SM_env\n",
    "from environment import random_normal_trunc\n",
    "from environment import eth_env\n",
    "from environment import SM_env_with_stale\n",
    "from environment import random_normal_trunc\n",
    "import mdptoolbox\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # or any {'0', '1', '2'}\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self, h_size, state_space_n, state_vector_n, action_space_n):\n",
    "\n",
    "        self.state_space_n = state_space_n\n",
    "        self.action_space_n = action_space_n\n",
    "        self.state_vector_n = state_vector_n\n",
    "\n",
    "        # The network recieves a state number from\n",
    "        # It then resizes it and processes it through four convolutional layers.\n",
    "        self.vectorIn = tf.placeholder(shape=[None, state_vector_n], dtype=tf.float32)\n",
    "        #print(self.scalarInput)\n",
    "        #self.vectorIn = tf.one_hot(self.scalarInput, state_space_n, dtype=tf.float32)\n",
    "        #print(self.vectorIn)\n",
    "        self.fc1 = tf.layers.dense(self.vectorIn, h_size, activation=tf.nn.relu)\n",
    "        #print(self.fc1)\n",
    "        self.fc2 = tf.layers.dense(self.fc1, h_size, activation=tf.nn.relu)\n",
    "        #print(self.fc2)\n",
    "\n",
    "        '''\n",
    "        self.imageIn = tf.reshape(self.scalarInput, shape=[-1, 84, 84, 3])\n",
    "        self.conv1 = slim.conv2d( \\\n",
    "            inputs=self.imageIn, num_outputs=32, kernel_size=[8, 8], stride=[4, 4], padding='VALID',\n",
    "            biases_initializer=None)\n",
    "        self.conv2 = slim.conv2d( \\\n",
    "            inputs=self.conv1, num_outputs=64, kernel_size=[4, 4], stride=[2, 2], padding='VALID',\n",
    "            biases_initializer=None)\n",
    "        self.conv3 = slim.conv2d( \\\n",
    "            inputs=self.conv2, num_outputs=64, kernel_size=[3, 3], stride=[1, 1], padding='VALID',\n",
    "            biases_initializer=None)\n",
    "        self.conv4 = slim.conv2d( \\\n",
    "            inputs=self.conv3, num_outputs=h_size, kernel_size=[7, 7], stride=[1, 1], padding='VALID',\n",
    "            biases_initializer=None)\n",
    "        '''\n",
    "\n",
    "        # We take the output from the final layer and split it into separate advantage and value streams.\n",
    "        #self.streamAC, self.streamVC = tf.split(self.conv4, 2, 3)\n",
    "        #self.streamA = slim.flatten(self.streamAC)\n",
    "        #self.streamV = slim.flatten(self.streamVC)\n",
    "\n",
    "        #print(self.fc2)\n",
    "\n",
    "        self.streamA, self.streamV = tf.split(self.fc2, 2, 1)\n",
    "        xavier_init = tf.contrib.layers.xavier_initializer()\n",
    "        self.AW = tf.Variable(xavier_init([h_size // 2, action_space_n]))\n",
    "        self.VW = tf.Variable(xavier_init([h_size // 2, 1]))\n",
    "        self.Advantage = tf.matmul(self.streamA, self.AW)\n",
    "        self.Value = tf.matmul(self.streamV, self.VW)\n",
    "        # Then combine them together to get our final Q-values.\n",
    "        self.Qout = self.Value + tf.subtract(self.Advantage, tf.reduce_mean(self.Advantage, axis=1, keep_dims=True))\n",
    "        #print(self.Qout)\n",
    "        self.predict = tf.argmax(self.Qout, 1)\n",
    "\n",
    "        # Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "        self.targetQ = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions, action_space_n, dtype=tf.float32)\n",
    "\n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.actions_onehot), axis=1)\n",
    "\n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        self.loss = tf.reduce_mean(self.td_error)\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)\n",
    "\n",
    "    def get_Q_table(self, sess, s):\n",
    "        Q = sess.run(self.Qout, feed_dict={self.vectorIn:[s]})\n",
    "        Q = np.reshape(Q, [-1])\n",
    "        return Q\n",
    "\n",
    "    def act_epsilon_greedy(self, sess, s, e = 0):\n",
    "\n",
    "        #legal_move_list = env.legal_move_list(s)\n",
    "        #legal_move_list = range(env._action_space_n)\n",
    "\n",
    "        if np.random.rand(1) < e:\n",
    "            a = np.random.choice(self.action_space_n)\n",
    "        else:\n",
    "            Q = self.get_Q_table(sess, s)\n",
    "            a = np.argmax(Q)\n",
    "\n",
    "            '''\n",
    "            #print(Q)\n",
    "            a = 0\n",
    "            val = -100000\n",
    "            for i in range(self.):\n",
    "                if (Q[i] > val):\n",
    "                    val = Q[i]\n",
    "                    a = i\n",
    "            '''\n",
    "\n",
    "        return a\n",
    "\n",
    "    def get_policy_table(self, sess, env):\n",
    "        policy = np.zeros(self.state_space_n, dtype = np.int32)\n",
    "        for i in range(0, self.state_space_n):\n",
    "            ss = env._index_to_vector(i)\n",
    "            policy[i] = self.act_epsilon_greedy(sess, ss, 0)\n",
    "        return policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_BLOCK = 20 # maximum hidden block of attacker\n",
    "rule = \"longest\" # \"longest\" -- bitcoin rule, \"GHOST\" -- GHOST rule\n",
    "h_size = 100 #The size of the final convolutional layer before splitting it into Advantage and Value streams.\n",
    "path = \"./btc_\" + rule + str(HIDDEN_BLOCK) + \"_\" + str(h_size) #The path to save our model to.\n",
    "know_alpha = True # if the agent knows the current alpha.\n",
    "# if know_alpha == True: path += \"know_alpha\"\n",
    "best_path = path + \"_cp/model_best.ckpt\" # best model path\n",
    "\n",
    "stale_rate = 0.0 # stale block rate, 0 means no stale block -- the classical selfish mining setting\n",
    "ALPHA = 0.4 # the hash power fraction of attacker\n",
    "GAMMA = 0.5 # the follower's fraction\n",
    "DEV = 0.0 # the alpha's fluctuation rate, 0 means fixed alpha\n",
    "know_alpha = True # if the agent knows the current alpha.\n",
    "random_process = \"iid\" # or \"brown\" -- brownian process\n",
    "interval = (0, 0.5) # the range of the alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state space size =  733\n"
     ]
    }
   ],
   "source": [
    "env = SM_env_with_stale(max_hidden_block = HIDDEN_BLOCK, attacker_fraction = ALPHA, follower_fraction = GAMMA, rule = rule, stale_rate=stale_rate, dev = DEV, know_alpha = know_alpha, random_interval=interval, random_process = random_process, frequency=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_3818191/2114258100.py:3: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_3818191/2538090048.py:10: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_3818191/2538090048.py:14: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /home/jvy5516/miniconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f0bfa1787d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f0bfa1787d0>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f0bfa1787d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f0bfa1787d0>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f0bfa1787d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f0bfa1787d0>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f0bfa1787d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f0bfa1787d0>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:From /tmp/ipykernel_3818191/2538090048.py:62: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f0bf93a0050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f0bf93a0050>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f0bf93a0050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f0bf93a0050>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f0bf93a0050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f0bf93a0050>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f0bf93a0050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f0bf93a0050>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:From /tmp/ipykernel_3818191/2114258100.py:6: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "h_size = 100 #The size of the final convolutional layer before splitting it into Advantage and Value streams.\n",
    "load_best_model = True # load a saved best model\n",
    "init = tf.global_variables_initializer()\n",
    "mainQN = Qnetwork(h_size, env._state_space_n, env._state_vector_n, env._action_space_n)\n",
    "targetQN = Qnetwork(h_size, env._state_space_n, env._state_vector_n, env._action_space_n)\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tra = 500\n",
    "seed_trial = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_file = 'episode_15/train_index.txt'\n",
    "index_pd = pd.read_table(index_file, sep=';', header=None)\n",
    "index_loc = index_pd[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Best Model..\n",
      "INFO:tensorflow:Restoring parameters from ./btc_longest20_100_cp/model_best.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n",
    "    if load_best_model == True:\n",
    "        print('Loading Best Model..')\n",
    "        saver.restore(sess, best_path)\n",
    "\n",
    "    reward_origin = []\n",
    "    for index in range(num_tra):\n",
    "        env.seed(index_loc[index])\n",
    "        s = env.reset()\n",
    "        s_0 = s\n",
    "        r_total = 0\n",
    "        for i in range(100):\n",
    "            a = mainQN.act_epsilon_greedy(sess, s, 0)\n",
    "            s, r, d, _ = env.step(s, a, move = True)\n",
    "            r_total += r\n",
    "            if s == s_0:\n",
    "                break\n",
    "        reward_origin.append(r_total)\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = 'summary_explain/15e/smooth_5.txt'\n",
    "summary_pd = pd.read_table(summary, sep=';', header=None)\n",
    "max_loc = summary_pd[0].values\n",
    "min_loc = summary_pd[1].values\n",
    "relative_reward = summary_pd[1].values\n",
    "f1 = summary_pd[2].values\n",
    "f2 = summary_pd[3].values\n",
    "start = 5\n",
    "end = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad = './episode_15_front/padding_index.txt'\n",
    "# pad_index = pd.read_table(pad, sep=';', header=None)\n",
    "# pad_num = pad_index[0].values\n",
    "# # max_loc -= pad_num\n",
    "# # min_loc -= pad_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "def random_run(choice_state,reward_origin,f1,f2):\n",
    "    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n",
    "        if load_best_model == True:\n",
    "            print('Loading Best Model..')\n",
    "            saver.restore(sess, best_path)\n",
    "\n",
    "        reward_random = []\n",
    "        for index in range(num_tra):\n",
    "            # print(index)\n",
    "            break_state = choice_state[index] - 1\n",
    "            # break_state = 180\n",
    "            flag = False\n",
    "            average = []\n",
    "            for t in range(seed_trial):\n",
    "                env.seed(index_loc[index])\n",
    "                s = env.reset()\n",
    "                s_0 = s\n",
    "                if flag:\n",
    "                    continue\n",
    "                r_total = 0\n",
    "                modifiable = False\n",
    "                for i in range(200):\n",
    "                    a = mainQN.act_epsilon_greedy(sess, s, 0)\n",
    "                    if (i==break_state):\n",
    "                        modifiable = True\n",
    "                        legal_state = env.legal_move_list(s)\n",
    "                        if a not in legal_state:\n",
    "                            a = legal_state[0]\n",
    "                        legal_state.remove(a)\n",
    "                        if len(legal_state)==0:\n",
    "                            flag = True\n",
    "                            break\n",
    "                        elif len(legal_state)==1:\n",
    "                            a = legal_state[0]\n",
    "                        elif len(legal_state)==2:\n",
    "                            a = choice(legal_state)\n",
    "                        # print(\"The action is: \", a)\n",
    "                    s, r, d, _ = env.step(s, a, move = True)\n",
    "                    r_total+=r\n",
    "                    # line = (\"The block index is \" + str(i) + \"         \"+ str(s[0]) + \";\" + str(s[1]) + \";\" + str(s[3]) + \";\")\n",
    "                    # print(line)\n",
    "                    if flag==False and s == s_0:\n",
    "                        break\n",
    "                    \n",
    "                    # if (i==break_state):\n",
    "                    #     env.seed(2021+index+100+t)\n",
    "                if flag==False and modifiable==True:\n",
    "                    average.append(abs(r_total-reward_origin[index]))\n",
    "\n",
    "            if len(average)==0:\n",
    "                r=-1\n",
    "            else:\n",
    "                r = sum(average) / len(average)\n",
    "                # r = max(average)\n",
    "            reward_random.append(r)\n",
    "        sess.close()\n",
    "    return reward_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Best Model..\n",
      "INFO:tensorflow:Restoring parameters from ./btc_longest20_100_cp/model_best.ckpt\n"
     ]
    }
   ],
   "source": [
    "reward_max = random_run(max_loc, reward_origin,f1,f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Best Model..\n",
      "INFO:tensorflow:Restoring parameters from ./btc_longest20_100_cp/model_best.ckpt\n"
     ]
    }
   ],
   "source": [
    "reward_min = random_run(min_loc, reward_origin,f1,f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Best Model..\n",
      "INFO:tensorflow:Restoring parameters from ./btc_longest20_100_cp/model_best.ckpt\n"
     ]
    }
   ],
   "source": [
    "loc1 = np.random.randint(start,end,size=[num_tra])\n",
    "# for i in range(len(loc1)):\n",
    "#     loc1[i] = choice([9,11,13,10,12])\n",
    "# loc1 -= pad_num\n",
    "# for i in range(len(loc1)):\n",
    "#     if loc1[i]==max_loc[i] or loc1[i]==min_loc[i]:\n",
    "#         legal_list = [5,6,7,8,9]\n",
    "#         legal_list.remove(max_loc[i])\n",
    "#         legal_list.remove(min_loc[i])\n",
    "#         loc1[i] = choice(legal_list)\n",
    "rand1 = random_run(loc1, reward_origin,f1,f2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Best Model..\n",
      "INFO:tensorflow:Restoring parameters from ./btc_longest20_100_cp/model_best.ckpt\n",
      "Loading Best Model..\n",
      "INFO:tensorflow:Restoring parameters from ./btc_longest20_100_cp/model_best.ckpt\n"
     ]
    }
   ],
   "source": [
    "summary = 'summary_explain/15e/concept_5.txt'\n",
    "summary_pd = pd.read_table(summary, sep=';', header=None)\n",
    "max_loc_1 = summary_pd[0].values\n",
    "min_loc_1 = summary_pd[1].values\n",
    "relative_reward = summary_pd[1].values\n",
    "reward_max_1 = random_run(max_loc_1, reward_origin,f1,f2)\n",
    "reward_min_1 = random_run(min_loc_1, reward_origin,f1,f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prob(reward_max, reward_min, rand1):\n",
    "    reward_max = np.array(reward_max)\n",
    "    reward_min = np.array(reward_min)\n",
    "    rand1 = np.array(rand1)\n",
    "\n",
    "    cnt = 0\n",
    "    num = 0\n",
    "    for i in range(len(reward_max)):\n",
    "        if (reward_max[i]!=-1 and rand1[i]!=-1 and reward_min[i]!=-1 and reward_max[i]!=rand1[i]):\n",
    "            cnt+=1\n",
    "            if reward_max[i] > rand1[i]:\n",
    "                num+=1\n",
    "    print(\"Most important vs rand\", num/cnt, \" num trajectories\", cnt)\n",
    "\n",
    "    cnt = 0\n",
    "    num = 0\n",
    "    for i in range(len(reward_max)):\n",
    "        if (reward_min[i]!=-1 and rand1[i]!=-1 and reward_max[i]!=-1 and reward_min[i]!=rand1[i]):\n",
    "            cnt+=1\n",
    "            if reward_min[i] < rand1[i]:\n",
    "                num+=1\n",
    "    print(\"Least important vs rand\", num/cnt, \" num trajectories\", cnt)\n",
    "\n",
    "    cnt = 0\n",
    "    num = 0\n",
    "    for i in range(len(reward_max)):\n",
    "        if (reward_max[i]!=-1 and reward_min[i]!=-1 and rand1[i]!=-1 and reward_min[i]!=reward_max[i]):\n",
    "            cnt+=1\n",
    "            if reward_max[i] > reward_min[i]:\n",
    "                num+=1\n",
    "    print(\"Most important vs Least important\", num/cnt, \" num trajectories\", cnt)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_prob(reward_max, reward_min, rand1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# # plt.hist(max_loc,bins=np.unique(max_loc), color = 'red', lw=0, alpha=0.4,label = 'max')\n",
    "# plt.hist(min_loc,bins=np.unique(min_loc), color = 'indigo', lw=0, alpha=0.4,label = 'min')\n",
    "# # plt.hist(loc1,bins=np.unique(loc1)+1, color = 'yellow', lw=0, alpha=0.2)\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept Max average gap:  1.9130800904714944\n",
      "Concept Min average gap:  1.221104905047221\n",
      "Saliency Max average gap:  1.652565800273599\n",
      "Saliency Min average gap:  1.4104399748585779\n",
      "Rand average gap:  1.6018372817503443\n"
     ]
    }
   ],
   "source": [
    "sum_ = 0\n",
    "cnt = 0\n",
    "for e in reward_max_1:\n",
    "    if e!=-1:\n",
    "        cnt+=1\n",
    "        sum_+=e\n",
    "print(\"Concept Max average gap: \", sum_/cnt)\n",
    "sum_ = 0\n",
    "cnt = 0\n",
    "for e in reward_min_1:\n",
    "    if e!=-1:\n",
    "        cnt+=1\n",
    "        sum_+=e\n",
    "print(\"Concept Min average gap: \", sum_/cnt)\n",
    "sum_ = 0\n",
    "cnt = 0\n",
    "for e in reward_max:\n",
    "    if e!=-1:\n",
    "        cnt+=1\n",
    "        sum_+=e\n",
    "print(\"Saliency Max average gap: \", sum_/cnt)\n",
    "sum_ = 0\n",
    "cnt = 0\n",
    "for e in reward_min:\n",
    "    if e!=-1:\n",
    "        cnt+=1\n",
    "        sum_+=e\n",
    "print(\"Saliency Min average gap: \", sum_/cnt)\n",
    "sum_ = 0\n",
    "cnt = 0\n",
    "for e in rand1:\n",
    "    if e!=-1:\n",
    "        cnt+=1\n",
    "        sum_+=e\n",
    "print(\"Rand average gap: \", sum_/cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Legal trajectories:  404  average gap:  1.6581671655537649\n",
      "Legal trajectories:  391  average gap:  1.652128709926842\n",
      "Legal trajectories:  423  average gap:  1.5360030787838805\n"
     ]
    }
   ],
   "source": [
    "sum_ = 0\n",
    "cnt = 0\n",
    "for e in reward_max:\n",
    "    if e!=-1:\n",
    "        cnt+=1\n",
    "        sum_+=e\n",
    "print(\"Legal trajectories: \", cnt, \" average gap: \", sum_/cnt)\n",
    "sum_ = 0\n",
    "cnt = 0\n",
    "for e in reward_min:\n",
    "    if e!=-1:\n",
    "        cnt+=1\n",
    "        sum_+=e\n",
    "print(\"Legal trajectories: \", cnt, \" average gap: \", sum_/cnt)\n",
    "sum_ = 0\n",
    "cnt = 0\n",
    "for e in rand1:\n",
    "    if e!=-1:\n",
    "        cnt+=1\n",
    "        sum_+=e\n",
    "print(\"Legal trajectories: \", cnt, \" average gap: \", sum_/cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conclude_state(num,loc):\n",
    "    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n",
    "        if load_best_model == True:\n",
    "            print('Loading Best Model..')\n",
    "            saver.restore(sess, best_path)\n",
    "\n",
    "        states = []\n",
    "        append_num = 0\n",
    "        for index in range(num):\n",
    "            env.seed(index_loc[index])\n",
    "            s = env.reset()\n",
    "            s_0 = s\n",
    "            r_total = 0\n",
    "            for i in range(50):\n",
    "                a = mainQN.act_epsilon_greedy(sess, s, 0)\n",
    "                if 5<=i<=9:\n",
    "                    to_append = [s[0],s[1],s[3]]\n",
    "                    states.append(to_append)\n",
    "                    append_num+=1\n",
    "                if i!=0 and s==s_0:\n",
    "                    break\n",
    "\n",
    "                \n",
    "                # if (i==loc[index]):\n",
    "                #     to_append = [s[0],s[1],s[3]]\n",
    "                #     states.append(to_append)\n",
    "                #     break\n",
    "                s, r, d, _ = env.step(s, a, move = True)\n",
    "\n",
    "    sess.close()\n",
    "    print(append_num)\n",
    "    x = np.array(states)\n",
    "    mask = np.unique(x, axis=0)\n",
    "    cnt = np.zeros(mask.shape[0])\n",
    "    c = 0\n",
    "    for c in range(mask.shape[0]):\n",
    "        tmp=0\n",
    "        for i in range(x.shape[0]):\n",
    "            if (x[i]==mask[c]).all():\n",
    "                tmp+=1\n",
    "        cnt[c] = tmp\n",
    "    for i in range(cnt.shape[0]):\n",
    "        print(mask[i], cnt[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Best Model..\n",
      "INFO:tensorflow:Restoring parameters from ./btc_longest20_100_cp/model_best.ckpt\n",
      "2500\n",
      "[0 1 0] 23.0\n",
      "[1 1 0] 191.0\n",
      "[1 1 2] 191.0\n",
      "[1 2 0] 11.0\n",
      "[2 1 0] 96.0\n",
      "[2 1 2] 145.0\n",
      "[2 2 0] 180.0\n",
      "[2 2 2] 180.0\n",
      "[2 3 0] 22.0\n",
      "[3 1 0] 21.0\n",
      "[3 1 2] 210.0\n",
      "[3 2 0] 193.0\n",
      "[3 2 2] 273.0\n",
      "[3 3 0] 99.0\n",
      "[3 3 1] 22.0\n",
      "[3 3 2] 99.0\n",
      "[3 4 0] 23.0\n",
      "[4 1 0] 63.0\n",
      "[4 1 2] 48.0\n",
      "[4 2 0] 75.0\n",
      "[4 2 2] 130.0\n",
      "[4 3 0] 100.0\n",
      "[4 3 2] 61.0\n",
      "[5 0 0] 1.0\n",
      "[5 1 0] 4.0\n",
      "[5 1 2] 7.0\n",
      "[5 2 0] 6.0\n",
      "[5 2 2] 15.0\n",
      "[5 3 0] 11.0\n"
     ]
    }
   ],
   "source": [
    "conclude_state(500,min_loc_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9358790697674406"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = np.array(reward_origin)\n",
    "a = total.mean()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_state(num):\n",
    "    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n",
    "        if load_best_model == True:\n",
    "            print('Loading Best Model..')\n",
    "            saver.restore(sess, best_path)\n",
    "\n",
    "        states = []\n",
    "        average = []\n",
    "        traj_num = 0\n",
    "        for index in range(num):\n",
    "            env.seed(index_loc[index])\n",
    "            s = env.reset()\n",
    "            s_0 = s\n",
    "            r_total = 0\n",
    "            flag=False\n",
    "            traj_flag = False\n",
    "            for i in range(50):\n",
    "                a = mainQN.act_epsilon_greedy(sess, s, 0)\n",
    "                if 5<=i<=9 and s[0]==3 and s[1]==2 and s[3]==2:\n",
    "                    flag = True\n",
    "                    if traj_flag==False:\n",
    "                        traj_num+=1\n",
    "                        traj_flag=True\n",
    "                s, r, d, _ = env.step(s, a, move = True)\n",
    "                r_total+=r\n",
    "                # print(s)\n",
    "                \n",
    "                if s==s_0:\n",
    "                    break\n",
    "            if flag:\n",
    "                average.append(r_total)\n",
    "    sess.close()\n",
    "    if len(average)!=0:\n",
    "        print(sum(average)/len(average))\n",
    "        print(traj_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Best Model..\n",
      "INFO:tensorflow:Restoring parameters from ./btc_longest20_100_cp/model_best.ckpt\n",
      "0.9341340829712927\n",
      "273\n"
     ]
    }
   ],
   "source": [
    "stat_state(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "def patch_state(num):\n",
    "    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n",
    "        if load_best_model == True:\n",
    "            print('Loading Best Model..')\n",
    "            saver.restore(sess, best_path)\n",
    "\n",
    "        states = []\n",
    "        average = []\n",
    "        traj_num = 0\n",
    "        for index in range(num):\n",
    "            env.seed(index_loc[index])\n",
    "            s = env.reset()\n",
    "            s_0 = s\n",
    "            r_total = 0\n",
    "            flag=False\n",
    "            traj_flag = False\n",
    "            for i in range(50):\n",
    "                a = mainQN.act_epsilon_greedy(sess, s, 0)\n",
    "                if 5<=i<=9 and s[0]==3 and s[1]==2 and s[3]==2:\n",
    "                    a = choice([0,2])\n",
    "                    flag = True\n",
    "                    if traj_flag==False:\n",
    "                        traj_num+=1\n",
    "                        traj_flag=True\n",
    "                s, r, d, _ = env.step(s, a, move = True)\n",
    "                r_total+=r\n",
    "                # print(s)\n",
    "                \n",
    "                if s==s_0:\n",
    "                    break\n",
    "            if flag:\n",
    "                average.append(r_total)\n",
    "    sess.close()\n",
    "    if len(average)!=0:\n",
    "        print(sum(average)/len(average))\n",
    "        print(traj_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Best Model..\n",
      "INFO:tensorflow:Restoring parameters from ./btc_longest20_100_cp/model_best.ckpt\n",
      "1.2228128460686565\n",
      "273\n"
     ]
    }
   ],
   "source": [
    "patch_state(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "def all_patch_state(num):\n",
    "    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n",
    "        if load_best_model == True:\n",
    "            print('Loading Best Model..')\n",
    "            saver.restore(sess, best_path)\n",
    "\n",
    "        states = []\n",
    "        average = []\n",
    "        traj_num = 0\n",
    "        for index in range(num):\n",
    "            env.seed(index_loc[500+index])\n",
    "            s = env.reset()\n",
    "            s_0 = s\n",
    "            r_total = 0\n",
    "            traj_flag = False\n",
    "            for i in range(50):\n",
    "                a = mainQN.act_epsilon_greedy(sess, s, 0)\n",
    "                if 5<=i<=9 and s[0]==3 and s[1]==2 and s[3]==2:\n",
    "                    a = choice([0,2])\n",
    "                if 5<=i<=9 and s[0]==2 and s[1]==1 and s[3]==2:\n",
    "                    a = choice([0,2])\n",
    "                s, r, d, _ = env.step(s, a, move = True)\n",
    "                r_total+=r\n",
    "                # print(s)\n",
    "                \n",
    "                if s==s_0:\n",
    "                    break\n",
    "            average.append(r_total)\n",
    "    sess.close()\n",
    "    if len(average)!=0:\n",
    "        print(sum(average)/len(average))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Best Model..\n",
      "INFO:tensorflow:Restoring parameters from ./btc_longest20_100_cp/model_best.ckpt\n",
      "0.8863790697674006\n"
     ]
    }
   ],
   "source": [
    "all_patch_state(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e409cc3a94ad8bb9234f76c4cd8bbe147f9fb06eb07002607ea27bc862dccc92"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('py37': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
